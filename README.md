Research Paper Q&A System (RAG)
This project implements a Retrieval-Augmented Generation (RAG) system that allows you to ask questions about a collection of research papers (PDFs) and receive answers generated by a Large Language Model (LLM), along with the sources from which the information was retrieved. The project uses local models (Ollama for the LLM).

Features - 
Document Ingestion: Loads PDF research papers, splits them into manageable chunks, and creates vector embeddings.

Vector Database: Utilizes ChromaDB to store and efficiently retrieve relevant document chunks based on semantic similarity.

Local LLM Integration: Leverages Ollama to run a powerful LLM (Llama3) locally for answer generation.

Contextual Answers: Generates answers grounded in the content of your research papers, reducing hallucinations.

Source Attribution: Provides links to the original source documents (paper names and page numbers) for verification.

Streamlit UI: A user-friendly web interface for interacting with the RAG system.

Technologies Used - 
Python: The core programming language.

Streamlit: For building the interactive web application.

LangChain: A framework for developing applications powered by language models, used for chaining components.

langchain-community: Provides integrations for various components (e.g., PyPDFLoader, Chroma, SentenceTransformerEmbeddings).

langchain-core: Core abstractions for LangChain.

langchain.llms.Ollama: For integrating with local Ollama models.

ChromaDB: An open-source embedding database, used for vector storage and retrieval.

Sentence-Transformers: For generating high-quality sentence embeddings (all-MiniLM-L6-v2).

PyPDFLoader / pdfplumber: For loading and parsing PDF documents.

Ollama: A platform for running large language models locally.
